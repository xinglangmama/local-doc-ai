"""é…ç½®ç®¡ç†æ¨¡å—

ç”¨äºç®¡ç†åº”ç”¨ç¨‹åºçš„é…ç½®é€‰é¡¹ï¼ŒåŒ…æ‹¬è®¾å¤‡é€‰æ‹©ã€æ¨¡å‹é…ç½®ç­‰ã€‚
"""

import torch
import logging
import yaml
import os
from typing import Dict, Any, Optional


class Config:
    """é…ç½®ç®¡ç†ç±»"""
    
    def __init__(self, config_file: str = "config.yaml"):
        self.config_file = config_file
        self.config_data = self._load_config()
        
        # åˆå§‹åŒ–æ—¥å¿—
        self._setup_logging()
    
    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½YAMLé…ç½®æ–‡ä»¶"""
        try:
            if os.path.exists(self.config_file):
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    return yaml.safe_load(f) or {}
            else:
                logging.warning(f"é…ç½®æ–‡ä»¶ {self.config_file} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                return self._get_default_config()
        except Exception as e:
            logging.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return self._get_default_config()
    
    def _get_default_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            "device": {"mode": "auto"},
            "model": {
                "qwen": {
                    "name": "Qwen/Qwen2.5-7B-Instruct",
                    "max_length": 8192,
                    "temperature": 0.7
                },
                "embedding": {
                    "name": "BAAI/bge-large-zh-v1.5",
                    "cache_folder": "./models",
                    "trust_remote_code": True
                }
            },
            "vector_db": {
                "type": "faiss",
                "index_type": "IndexFlatIP",
                "dimension": 1024
            },
            "chat": {
                "max_history": 10,
                "context_window": 4000
            },
            "document": {
                "chunk_size": 500,
                "chunk_overlap": 50,
                "supported_formats": [".txt", ".pdf", ".docx", ".md"]
            },
            "app": {
                "title": "æœ¬åœ°æ–‡æ¡£AIåŠ©æ‰‹",
                "page_icon": "ğŸ¤–",
                "layout": "wide"
            },
            "logging": {
                "level": "INFO",
                "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
            },
            "cache": {
                "model_cache_size": 2,
                "clear_on_exit": False
            }
        }
    
    def save_config(self):
        """ä¿å­˜é…ç½®åˆ°YAMLæ–‡ä»¶"""
        try:
            with open(self.config_file, 'w', encoding='utf-8') as f:
                yaml.dump(self.config_data, f, default_flow_style=False, 
                         allow_unicode=True, sort_keys=False)
            logging.info(f"é…ç½®å·²ä¿å­˜åˆ° {self.config_file}")
        except Exception as e:
            logging.error(f"ä¿å­˜é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
    
    # å±æ€§è®¿é—®å™¨
    @property
    def device_mode(self) -> str:
        return self.config_data.get("device", {}).get("mode", "auto")
    
    @property
    def qwen_model_name(self) -> str:
        return self.config_data.get("model", {}).get("qwen", {}).get("name", "Qwen/Qwen2.5-7B-Instruct")
    
    @property
    def qwen_max_length(self) -> int:
        return self.config_data.get("model", {}).get("qwen", {}).get("max_length", 8192)
    
    @property
    def qwen_temperature(self) -> float:
        return self.config_data.get("model", {}).get("qwen", {}).get("temperature", 0.7)
    
    @property
    def embedding_model_name(self) -> str:
        return self.config_data.get("model", {}).get("embedding", {}).get("name", "BAAI/bge-large-zh-v1.5")
    
    @property
    def embedding_cache_folder(self) -> str:
        return self.config_data.get("model", {}).get("embedding", {}).get("cache_folder", "./models")
    
    @property
    def vector_db_type(self) -> str:
        return self.config_data.get("vector_db", {}).get("type", "faiss")
    
    @property
    def vector_dimension(self) -> int:
        return self.config_data.get("vector_db", {}).get("dimension", 1024)
    
    @property
    def max_chat_history(self) -> int:
        return self.config_data.get("chat", {}).get("max_history", 10)
    
    @property
    def context_window(self) -> int:
        return self.config_data.get("chat", {}).get("context_window", 4000)
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—é…ç½®"""
        log_config = self.config_data.get("logging", {})
        level = getattr(logging, log_config.get("level", "INFO").upper())
        format_str = log_config.get("format", "%(asctime)s - %(name)s - %(levelname)s - %(message)s")
        
        logging.basicConfig(
            level=level,
            format=format_str,
            handlers=[
                logging.StreamHandler()
            ]
        )
    
    def _get_device(self) -> str:
        """æ ¹æ®é…ç½®å’Œç¡¬ä»¶æƒ…å†µè‡ªåŠ¨é€‰æ‹©è®¾å¤‡"""
        mode = self.device_mode
        if mode == "cpu":
            return "cpu"
        elif mode == "cuda":
            if torch.cuda.is_available():
                return "cuda"
            else:
                logging.warning("è¯·æ±‚ä½¿ç”¨CUDAä½†CUDAä¸å¯ç”¨ï¼Œå›é€€åˆ°CPU")
                return "cpu"
        else:  # autoæ¨¡å¼
            return "cuda" if torch.cuda.is_available() else "cpu"
    
    @property
    def device(self) -> str:
        """è·å–å½“å‰è®¾å¤‡"""
        return self._get_device()
    
    def set_device_mode(self, mode: str):
        """è®¾ç½®è®¾å¤‡æ¨¡å¼"""
        if mode not in ["auto", "cuda", "cpu"]:
            raise ValueError(f"æ— æ•ˆçš„è®¾å¤‡æ¨¡å¼: {mode}")
        
        # æ›´æ–°é…ç½®æ•°æ®
        if "device" not in self.config_data:
            self.config_data["device"] = {}
        self.config_data["device"]["mode"] = mode
        
        # ä¿å­˜é…ç½®
        self.save_config()
        
        current_device = self._get_device()
        logging.info(f"è®¾å¤‡æ¨¡å¼å·²è®¾ç½®ä¸º: {mode}, å½“å‰ä½¿ç”¨è®¾å¤‡: {current_device}")
    
    def get_device_info(self) -> dict:
        """è·å–è®¾å¤‡ä¿¡æ¯"""
        current_device = self._get_device()
        info = {
            "device_mode": self.device_mode,
            "current_device": current_device,
            "cuda_available": torch.cuda.is_available()
        }
        
        if torch.cuda.is_available():
            info["cuda_device_count"] = torch.cuda.device_count()
            info["cuda_device_name"] = torch.cuda.get_device_name(0)
            info["cuda_memory_total"] = f"{torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB"
        
        return info
    
    def update_config(self, section: str, key: str, value: Any):
        """æ›´æ–°é…ç½®é¡¹"""
        if section not in self.config_data:
            self.config_data[section] = {}
        self.config_data[section][key] = value
        self.save_config()
    
    def get_config(self, section: str, key: str = None, default=None):
        """è·å–é…ç½®é¡¹"""
        section_data = self.config_data.get(section, {})
        if key is None:
            return section_data
        return section_data.get(key, default)


# åˆ›å»ºå…¨å±€é…ç½®å®ä¾‹
config = Config()