"""æ–‡æ¡£å¤„ç†æ¨¡å—
åŒ…å«æ–‡æ¡£æ–‡æœ¬æå–ã€æ–‡æœ¬åˆ†å‰²ã€å‘é‡å­˜å‚¨ç­‰åŠŸèƒ½"""

import streamlit as st
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.schema import Document
from PyPDF2 import PdfReader
from docx import Document as DocxDocument
from local_embeddings import LocalEmbeddings
import os

# å…¨å±€å˜é‡ï¼Œç”¨äºå­˜å‚¨å¤„ç†åçš„æ–‡æ¡£å—
processed_chunks = []

def get_embeddings():
    """
    è·å–åµŒå…¥æ¨¡å‹å®ä¾‹ï¼Œä½¿ç”¨Streamlit session_stateé¿å…é‡å¤åˆå§‹åŒ–
    """
    if "embeddings_instance" not in st.session_state:
        st.session_state.embeddings_instance = LocalEmbeddings()
    return st.session_state.embeddings_instance


def get_pdf_text(pdf_docs):
    """
    ä»ä¸Šä¼ çš„PDFæ–‡ä»¶åˆ—è¡¨ä¸­æå–æ‰€æœ‰æ–‡æœ¬å†…å®¹
    å‚æ•°: pdf_docs - PDFæ–‡ä»¶åˆ—è¡¨
    è¿”å›: åˆå¹¶åçš„æ–‡æœ¬å­—ç¬¦ä¸²
    """
    text = ""  # åˆå§‹åŒ–ç©ºå­—ç¬¦ä¸²ç”¨äºå­˜å‚¨æå–çš„æ–‡æœ¬
    # éå†æ¯ä¸ªPDFæ–‡ä»¶
    for pdf in pdf_docs:
        try:
            pdf_reader = PdfReader(pdf)  # åˆ›å»ºPDFé˜…è¯»å™¨å¯¹è±¡
            # éå†PDFçš„æ¯ä¸€é¡µ
            for page_num, page in enumerate(pdf_reader.pages):
                try:
                    page_text = page.extract_text()  # æå–é¡µé¢æ–‡æœ¬
                    if page_text:
                        text += page_text + "\n"  # æ·»åŠ æ¢è¡Œç¬¦åˆ†éš”é¡µé¢
                except Exception as e:
                    st.warning(f"âš ï¸ PDFç¬¬{page_num+1}é¡µæ–‡æœ¬æå–å¤±è´¥: {str(e)}")
                    continue
            
            if not text.strip():
                st.warning(f"âš ï¸ PDFæ–‡ä»¶ {pdf.name} ä¸­æœªæå–åˆ°ä»»ä½•æ–‡æœ¬å†…å®¹")
        except Exception as e:
            st.error(f"âŒ è¯»å–PDFæ–‡ä»¶ {pdf.name} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            continue
    
    return text  # è¿”å›åˆå¹¶åçš„æ‰€æœ‰æ–‡æœ¬


def get_docx_text(docx_docs):
    """
    ä»ä¸Šä¼ çš„Wordæ–‡æ¡£åˆ—è¡¨ä¸­æå–æ‰€æœ‰æ–‡æœ¬å†…å®¹
    å‚æ•°: docx_docs - Wordæ–‡æ¡£æ–‡ä»¶åˆ—è¡¨
    è¿”å›: åˆå¹¶åçš„æ–‡æœ¬å­—ç¬¦ä¸²
    """
    text = ""  # åˆå§‹åŒ–ç©ºå­—ç¬¦ä¸²ç”¨äºå­˜å‚¨æå–çš„æ–‡æœ¬
    # éå†æ¯ä¸ªWordæ–‡æ¡£æ–‡ä»¶
    for docx_file in docx_docs:
        try:
            doc = docx.Document(docx_file)  # åˆ›å»ºWordæ–‡æ¡£å¯¹è±¡
            doc_text = ""
            # éå†æ–‡æ¡£ä¸­çš„æ¯ä¸ªæ®µè½
            for para in doc.paragraphs:
                if para.text.strip():  # åªæ·»åŠ éç©ºæ®µè½
                    doc_text += para.text + "\n"  # æå–æ®µè½æ–‡æœ¬å¹¶æ·»åŠ æ¢è¡Œç¬¦
            
            if doc_text.strip():
                text += doc_text
            else:
                st.warning(f"âš ï¸ Wordæ–‡æ¡£ {docx_file.name} ä¸­æœªæå–åˆ°ä»»ä½•æ–‡æœ¬å†…å®¹")
                
        except Exception as e:
            st.error(f"âŒ è¯»å–Wordæ–‡æ¡£ {docx_file.name} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            continue
    
    return text  # è¿”å›åˆå¹¶åçš„æ‰€æœ‰æ–‡æœ¬


def get_text_chunks(text):
    """
    å°†é•¿æ–‡æœ¬åˆ†å‰²æˆè¾ƒå°çš„æ–‡æœ¬å—ï¼Œä¾¿äºå‘é‡åŒ–å’Œæ£€ç´¢
    å‚æ•°: text - éœ€è¦åˆ†å‰²çš„åŸå§‹æ–‡æœ¬å­—ç¬¦ä¸²
    è¿”å›: æ–‡æœ¬å—åˆ—è¡¨
    """
    if not text or not text.strip():
        st.warning("âš ï¸ è¾“å…¥æ–‡æœ¬ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œåˆ†å—å¤„ç†")
        return []
    
    try:
        # åˆ›å»ºé€’å½’å­—ç¬¦æ–‡æœ¬åˆ†å‰²å™¨ï¼Œç”¨äºæ™ºèƒ½åˆ†å‰²æ–‡æœ¬
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=300,        # æ¯ä¸ªæ–‡æœ¬å—çš„æœ€å¤§å­—ç¬¦æ•°
            chunk_overlap=50,      # ç›¸é‚»æ–‡æœ¬å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°ï¼Œä¿æŒä¸Šä¸‹æ–‡è¿è´¯æ€§
            length_function=len     # ä½¿ç”¨å­—ç¬¦é•¿åº¦ä½œä¸ºåˆ†å‰²ä¾æ®
        )
        # ä½¿ç”¨åˆ†å‰²å™¨å°†æ–‡æœ¬åˆ†å‰²æˆå¤šä¸ªå—
        chunks = text_splitter.split_text(text)
        
        # è¿‡æ»¤æ‰ç©ºçš„æˆ–è¿‡çŸ­çš„æ–‡æœ¬å—
        valid_chunks = [chunk.strip() for chunk in chunks if chunk.strip() and len(chunk.strip()) > 10]
        
        if not valid_chunks:
            st.warning("âš ï¸ æ–‡æœ¬åˆ†å—åæ²¡æœ‰æœ‰æ•ˆçš„æ–‡æœ¬å—")
            return []
        
        return valid_chunks  # è¿”å›åˆ†å‰²åçš„æœ‰æ•ˆæ–‡æœ¬å—åˆ—è¡¨
    except Exception as e:
        st.error(f"âŒ æ–‡æœ¬åˆ†å—è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        return []


def get_vectorstore(text_chunks, source_filename="æœªçŸ¥æ–‡æ¡£", append_mode=True):
    """
    å°†æ–‡æœ¬å—è½¬æ¢ä¸ºå‘é‡å¹¶åˆ›å»ºFAISSå‘é‡æ•°æ®åº“
    å‚æ•°: 
        text_chunks - æ–‡æœ¬å—åˆ—è¡¨
        source_filename - æºæ–‡æ¡£æ–‡ä»¶å
        append_mode - æ˜¯å¦è¿½åŠ æ¨¡å¼ï¼ˆTrue: è¿½åŠ åˆ°ç°æœ‰æ•°æ®åº“ï¼ŒFalse: åˆ›å»ºæ–°æ•°æ®åº“ï¼‰
    è¿”å›: FAISSå‘é‡å­˜å‚¨å¯¹è±¡
    """
    if not text_chunks:
        st.warning("âš ï¸ æ–‡æœ¬å—åˆ—è¡¨ä¸ºç©ºï¼Œæ— æ³•åˆ›å»ºå‘é‡æ•°æ®åº“")
        return None
    
    # è¿‡æ»¤æ‰ç©ºçš„æ–‡æœ¬å—
    valid_chunks = [chunk.strip() for chunk in text_chunks if chunk and chunk.strip()]
    
    if not valid_chunks:
        st.warning("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æœ¬å—ï¼Œæ— æ³•åˆ›å»ºå‘é‡æ•°æ®åº“")
        return None
    
    try:
        # è·å–å…±äº«çš„åµŒå…¥æ¨¡å‹å®ä¾‹ï¼Œé¿å…é‡å¤åˆå§‹åŒ–
        st.info("ğŸ“¡ æ­£åœ¨è·å–åµŒå…¥æ¨¡å‹...")
        embeddings = get_embeddings()
        
        # æ˜¾ç¤ºæ–‡æœ¬å—æ ·æœ¬ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        st.info(f"ğŸ“ æ–‡æœ¬å—æ ·æœ¬: {valid_chunks[0][:100]}...")
        
        # åˆ›å»ºDocumentå¯¹è±¡åˆ—è¡¨ï¼Œä¸ºæ¯ä¸ªæ–‡æœ¬å—æ·»åŠ metadata
        st.info("ğŸ“„ æ­£åœ¨åˆ›å»ºDocumentå¯¹è±¡å¹¶æ·»åŠ metadata...")
        documents = [
            Document(page_content=chunk, metadata={"source": source_filename, "chunk_id": i})
            for i, chunk in enumerate(valid_chunks)
        ]
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç°æœ‰çš„å‘é‡æ•°æ®åº“ä¸”å¯ç”¨è¿½åŠ æ¨¡å¼
        if append_mode and check_vectorstore_exists():
            st.info("ğŸ”„ æ£€æµ‹åˆ°ç°æœ‰å‘é‡æ•°æ®åº“ï¼Œæ­£åœ¨è¿½åŠ æ–°å†…å®¹...")
            try:
                # åŠ è½½ç°æœ‰çš„å‘é‡æ•°æ®åº“
                existing_vectorstore = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)
                
                # åˆ›å»ºæ–°æ–‡æ¡£çš„å‘é‡æ•°æ®åº“
                new_vectorstore = FAISS.from_documents(documents, embeddings)
                
                # å°†æ–°å‘é‡æ•°æ®åº“åˆå¹¶åˆ°ç°æœ‰æ•°æ®åº“ä¸­
                existing_vectorstore.merge_from(new_vectorstore)
                vectorstore = existing_vectorstore
                
                st.success(f"âœ… æˆåŠŸè¿½åŠ  {len(valid_chunks)} ä¸ªæ–°æ–‡æœ¬å—åˆ°ç°æœ‰å‘é‡æ•°æ®åº“ï¼")
            except Exception as e:
                st.warning(f"âš ï¸ åŠ è½½ç°æœ‰æ•°æ®åº“å¤±è´¥ï¼Œå°†åˆ›å»ºæ–°æ•°æ®åº“: {str(e)}")
                # å¦‚æœåŠ è½½å¤±è´¥ï¼Œåˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“
                st.info("ğŸ”„ æ­£åœ¨åˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“...")
                vectorstore = FAISS.from_documents(documents, embeddings)
                st.success(f"âœ… æ–°å‘é‡æ•°æ®åº“åˆ›å»ºæˆåŠŸï¼å…±å¤„ç†äº† {len(valid_chunks)} ä¸ªæ–‡æœ¬å—ã€‚")
        else:
            # åˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“
            st.info(f"ğŸ”„ æ­£åœ¨åˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“ï¼Œå¤„ç† {len(valid_chunks)} ä¸ªæ–‡æœ¬å—...")
            vectorstore = FAISS.from_documents(documents, embeddings)
            st.success(f"âœ… æ–°å‘é‡æ•°æ®åº“åˆ›å»ºæˆåŠŸï¼å…±å¤„ç†äº† {len(valid_chunks)} ä¸ªæ–‡æœ¬å—ã€‚")
        
        # ä¿å­˜å‘é‡æ•°æ®åº“åˆ°æœ¬åœ°æ–‡ä»¶ï¼Œå®ç°æŒä¹…åŒ–å­˜å‚¨
        st.info("ğŸ’¾ æ­£åœ¨ä¿å­˜å‘é‡æ•°æ®åº“åˆ°æœ¬åœ°...")
        vectorstore.save_local("faiss_index")
        st.success("âœ… å‘é‡æ•°æ®åº“å·²ä¿å­˜åˆ°æœ¬åœ°ï¼")
        
        return vectorstore  # è¿”å›åˆ›å»ºçš„å‘é‡å­˜å‚¨å¯¹è±¡
    except Exception as e:
        st.error(f"âŒ åˆ›å»ºå‘é‡æ•°æ®åº“æ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}")
        st.error(f"é”™è¯¯è¯¦æƒ…: {type(e).__name__}")
        import traceback
        st.error(f"å®Œæ•´é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        return None


def load_vectorstore():
    """
    ä»æœ¬åœ°æ–‡ä»¶åŠ è½½å·²ä¿å­˜çš„FAISSå‘é‡æ•°æ®åº“
    è¿”å›: FAISSå‘é‡å­˜å‚¨å¯¹è±¡ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨åˆ™è¿”å›None
    """
    try:
        # è·å–å…±äº«çš„åµŒå…¥æ¨¡å‹å®ä¾‹ï¼ˆå¿…é¡»ä¸ä¿å­˜æ—¶ä½¿ç”¨çš„æ¨¡å‹ä¸€è‡´ï¼‰
        embeddings = get_embeddings()
        # ä»æœ¬åœ°æ–‡ä»¶åŠ è½½å‘é‡æ•°æ®åº“
        vectorstore = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)
        return vectorstore
    except Exception as e:
        # å¦‚æœåŠ è½½å¤±è´¥ï¼ˆå¦‚æ–‡ä»¶ä¸å­˜åœ¨ï¼‰ï¼Œè¿”å›None
        return None


def process_uploaded_files(uploaded_files, append_mode=False):
    """
    å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶ï¼Œæå–æ–‡æœ¬å¹¶åˆ›å»ºå‘é‡æ•°æ®åº“
    
    Args:
        uploaded_files: Streamlitä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨
        append_mode: æ˜¯å¦ä¸ºè¿½åŠ æ¨¡å¼ï¼ŒTrueè¡¨ç¤ºè¿½åŠ åˆ°ç°æœ‰æ•°æ®åº“ï¼ŒFalseè¡¨ç¤ºåˆ›å»ºæ–°æ•°æ®åº“
        
    Returns:
        FAISSå‘é‡æ•°æ®åº“å¯¹è±¡ï¼Œå¦‚æœå¤„ç†å¤±è´¥åˆ™è¿”å›None
    """
    if not uploaded_files:
        st.warning("ğŸ“‹ è¯·å…ˆä¸Šä¼ æ–‡æ¡£ï¼")
        return None
    
    # æ˜¾ç¤ºå¤„ç†è¿›åº¦æŒ‡ç¤ºå™¨
    with st.spinner("ğŸ”„ æ­£åœ¨å¤„ç†æ–‡æ¡£ï¼Œè¯·ç¨å€™..."):
        raw_text = ""  # åˆå§‹åŒ–å˜é‡å­˜å‚¨æ‰€æœ‰æå–çš„æ–‡æœ¬
        file_names = []  # å­˜å‚¨æ–‡ä»¶ååˆ—è¡¨
        
        # ä»ä¸Šä¼ æ–‡ä»¶ä¸­ç­›é€‰å‡ºPDFæ–‡ä»¶
        pdf_files = [f for f in uploaded_files if f.type == "application/pdf"]
        # ä»ä¸Šä¼ æ–‡ä»¶ä¸­ç­›é€‰å‡ºWordæ–‡æ¡£æ–‡ä»¶
        docx_files = [f for f in uploaded_files if f.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document"]

        # å¦‚æœæœ‰PDFæ–‡ä»¶ï¼Œæå–å…¶ä¸­çš„æ–‡æœ¬å†…å®¹
        if pdf_files:
            raw_text += get_pdf_text(pdf_files)
            file_names.extend([f.name for f in pdf_files])
        # å¦‚æœæœ‰Wordæ–‡æ¡£ï¼Œæå–å…¶ä¸­çš„æ–‡æœ¬å†…å®¹
        if docx_files:
            raw_text += get_docx_text(docx_files)
            file_names.extend([f.name for f in docx_files])

        # æ£€æŸ¥æ˜¯å¦æˆåŠŸæå–åˆ°æ–‡æœ¬å†…å®¹
        if not raw_text.strip():
            st.error("âŒ æœªèƒ½ä»ä¸Šä¼ çš„æ–‡æ¡£ä¸­æå–åˆ°æœ‰æ•ˆæ–‡æœ¬å†…å®¹ï¼è¯·æ£€æŸ¥æ–‡æ¡£æ ¼å¼æˆ–å†…å®¹ã€‚")
            return None
        
        # æ˜¾ç¤ºæå–åˆ°çš„æ–‡æœ¬ç»Ÿè®¡ä¿¡æ¯
        st.info(f"ğŸ“„ æˆåŠŸæå–æ–‡æœ¬ï¼Œæ€»å­—ç¬¦æ•°: {len(raw_text)}")
        
        # åˆå¹¶æ‰€æœ‰æ–‡ä»¶åä½œä¸ºæºæ–‡æ¡£æ ‡è¯†
        source_filename = ", ".join(file_names) if file_names else "æœªçŸ¥æ–‡æ¡£"

        # å°†æå–çš„åŸå§‹æ–‡æœ¬åˆ†å‰²æˆå°å—ï¼Œä¾¿äºå‘é‡åŒ–å¤„ç†
        text_chunks = get_text_chunks(raw_text)
        
        # éªŒè¯æ–‡æœ¬åˆ†å—ç»“æœ
        if not text_chunks:
            st.error("âŒ æ–‡æœ¬åˆ†å—å¤±è´¥ï¼Œæ— æ³•åˆ›å»ºå‘é‡æ•°æ®åº“ï¼")
            return None
        
        st.info(f"ğŸ“ æ–‡æœ¬å·²åˆ†å‰²ä¸º {len(text_chunks)} ä¸ªå—")

        # å°†æ–‡æœ¬å—è½¬æ¢ä¸ºå‘é‡å¹¶åˆ›å»ºå¯æœç´¢çš„å‘é‡æ•°æ®åº“
        vectorstore = get_vectorstore(text_chunks, source_filename, append_mode)
        
        if vectorstore is None:
            st.error("âŒ æ–‡æ¡£å¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šä¼ çš„æ–‡ä»¶ï¼")
            return None
        
        return vectorstore


def check_vectorstore_exists():
    """
    æ£€æŸ¥æœ¬åœ°å‘é‡æ•°æ®åº“æ˜¯å¦å­˜åœ¨
    è¿”å›: bool - æ•°æ®åº“æ˜¯å¦å­˜åœ¨
    """
    return os.path.exists("faiss_index")


def get_vectorstore_info():
    """
    è·å–å‘é‡æ•°æ®åº“çš„è¯¦ç»†ä¿¡æ¯
    
    Returns:
        dict: åŒ…å«chunkä¸ªæ•°å’Œæ–‡æ¡£æ¥æºåˆ—è¡¨çš„å­—å…¸ï¼Œå¦‚æœæ•°æ®åº“ä¸å­˜åœ¨åˆ™è¿”å›None
    """
    if not check_vectorstore_exists():
        return None
    
    try:
        # åŠ è½½å‘é‡æ•°æ®åº“
        embeddings = LocalEmbeddings()
        vectorstore = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)
        
        # è·å–æ‰€æœ‰æ–‡æ¡£çš„metadata
        docstore = vectorstore.docstore
        all_docs = [docstore.search(doc_id) for doc_id in docstore._dict.keys()]
        
        # ç»Ÿè®¡chunkä¸ªæ•°
        chunk_count = len(all_docs)
        
        # æå–æ–‡æ¡£æ¥æºåˆ—è¡¨
        source_files = set()
        for doc in all_docs:
            if hasattr(doc, 'metadata') and 'source' in doc.metadata:
                source_files.add(doc.metadata['source'])
        
        return {
            'chunk_count': chunk_count,
            'source_files': list(source_files)
        }
    
    except Exception as e:
        print(f"è·å–å‘é‡æ•°æ®åº“ä¿¡æ¯æ—¶å‡ºé”™: {e}")
        return None