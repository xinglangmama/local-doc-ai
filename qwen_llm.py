# -*- coding: utf-8 -*-
"""
Qwen2.5-1.5B-Instructæ¨¡å‹çš„LangChainåŒ…è£…å™¨ (ä½¿ç”¨ModelScope)
æä¾›åŸºäºModelScopeçš„Qwen2.5-1.5B-Instructæ¨¡å‹ä¸LangChainæ¡†æ¶çš„é›†æˆ
"""

from modelscope import AutoTokenizer, AutoModelForCausalLM
import torch
from typing import Optional, List, Any
from langchain_core.language_models.llms import BaseLLM
from langchain_core.callbacks.manager import CallbackManagerForLLMRun
from langchain_core.outputs import LLMResult, Generation
from pydantic import Field
import logging
import os
import shutil
from config import config

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# æ¨¡å—çº§åˆ«çš„æ¨¡å‹ç¼“å­˜
_MODEL_CACHE = {
    'model': None,
    'tokenizer': None,
    'initialized': False
}


class QwenLLM(BaseLLM):
    """
    åŸºäºQwen2.5-1.5B-Instructçš„æœ¬åœ°LLMå®ç° (ä½¿ç”¨ModelScope)
    ä½¿ç”¨æ¨¡å—çº§åˆ«çš„æ¨¡å‹ç¼“å­˜é¿å…é‡å¤åˆå§‹åŒ–
    """
    
    model_name: str = Field(default_factory=lambda: config.qwen_model_name, description="æ¨¡å‹åç§°")
    max_length: int = Field(default_factory=lambda: config.qwen_max_length, description="æœ€å¤§ç”Ÿæˆé•¿åº¦")
    temperature: float = Field(default_factory=lambda: config.qwen_temperature, description="ç”Ÿæˆæ¸©åº¦")
    device: Any = Field(default=None, description="è®¡ç®—è®¾å¤‡")
    
    def __init__(self, **kwargs):
        """åˆå§‹åŒ–Qwen LLM
        
        Args:
            **kwargs: å…¶ä»–å‚æ•°
        """
        # æ­£ç¡®è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__(**kwargs)
        
        # è®¾ç½®è®¾å¤‡ï¼ˆä½¿ç”¨é…ç½®ç³»ç»Ÿï¼‰
        self.device = torch.device(config.device)
        
        # å¦‚æœæ¨¡å‹è¿˜æœªåˆå§‹åŒ–ï¼Œåˆ™åˆå§‹åŒ–ç¼“å­˜æ¨¡å‹
        if not _MODEL_CACHE['initialized']:
            self._initialize_model()
    
    @property
    def model(self):
        """è·å–ç¼“å­˜çš„æ¨¡å‹å®ä¾‹"""
        return _MODEL_CACHE['model']
    
    @property
    def tokenizer(self):
        """è·å–ç¼“å­˜çš„tokenizerå®ä¾‹"""
        return _MODEL_CACHE['tokenizer']
    
    @classmethod
    def clear_instance(cls):
        """æ¸…ç†ç¼“å­˜æ¨¡å‹å®ä¾‹ï¼Œé‡Šæ”¾æ¨¡å‹èµ„æº"""
        try:
            if _MODEL_CACHE['model'] is not None:
                del _MODEL_CACHE['model']
                _MODEL_CACHE['model'] = None
                
            if _MODEL_CACHE['tokenizer'] is not None:
                del _MODEL_CACHE['tokenizer']
                _MODEL_CACHE['tokenizer'] = None
                
            # æ¸…ç†CUDAç¼“å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                
            _MODEL_CACHE['initialized'] = False
            logger.info("ğŸ§¹ å·²æ¸…ç†Qwenç¼“å­˜æ¨¡å‹å®ä¾‹å’ŒCUDAç¼“å­˜")
        except Exception as e:
            logger.warning(f"æ¸…ç†æ¨¡å‹å®ä¾‹æ—¶å‡ºç°è­¦å‘Š: {e}")
    
    def _initialize_model(self):
        """åˆå§‹åŒ–ç¼“å­˜çš„Qwenæ¨¡å‹ (ä½¿ç”¨ModelScope)"""
        if _MODEL_CACHE['initialized']:
            logger.info("ğŸ”„ ä½¿ç”¨å·²å­˜åœ¨çš„Qwenç¼“å­˜æ¨¡å‹å®ä¾‹")
            return
            
        import time
        max_retries = 3
        retry_delay = 2
        
        for attempt in range(max_retries):
            try:
                logger.info(f"æ­£åœ¨ä»ModelScopeåŠ è½½Qwenæ¨¡å‹: qwen/Qwen2.5-1.5B-Instruct (å°è¯• {attempt + 1}/{max_retries})")
                
                # æ¸…ç†CUDAç¼“å­˜ä»¥é‡Šæ”¾æ˜¾å­˜
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    logger.info("ğŸ§¹ å·²æ¸…ç†CUDAç¼“å­˜")
                
                # åŠ è½½tokenizer
                _MODEL_CACHE['tokenizer'] = AutoTokenizer.from_pretrained(
                    "qwen/Qwen2.5-1.5B-Instruct",
                    trust_remote_code=True,
                    revision='master',
                    local_files_only=False,
                    force_download=False
                )
                
                # ä½¿ç”¨ä¼˜åŒ–é…ç½®åŠ è½½æ¨¡å‹ä»¥èŠ‚çœæ˜¾å­˜
                _MODEL_CACHE['model'] = AutoModelForCausalLM.from_pretrained(
                    "qwen/Qwen2.5-1.5B-Instruct",
                    torch_dtype=torch.float16,  # ä½¿ç”¨åŠç²¾åº¦
                    low_cpu_mem_usage=True,     # é™ä½CPUå†…å­˜ä½¿ç”¨
                    trust_remote_code=True,
                    revision='master',
                    local_files_only=False,
                    force_download=False
                )
                
                # æ‰‹åŠ¨ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡
                _MODEL_CACHE['model'] = _MODEL_CACHE['model'].to(self.device)
                
                # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
                _MODEL_CACHE['model'].eval()
                _MODEL_CACHE['initialized'] = True
                
                logger.info("âœ… Qwenç¼“å­˜æ¨¡å‹ä»ModelScopeåŠ è½½æˆåŠŸ")
                return
                
            except PermissionError as e:
                if "å¦ä¸€ä¸ªç¨‹åºæ­£åœ¨ä½¿ç”¨æ­¤æ–‡ä»¶" in str(e) or "WinError 32" in str(e):
                    if attempt < max_retries - 1:
                        logger.warning(f"âš ï¸ æ–‡ä»¶è¢«å ç”¨ï¼Œç­‰å¾… {retry_delay} ç§’åé‡è¯•...")
                        time.sleep(retry_delay)
                        retry_delay *= 2  # æŒ‡æ•°é€€é¿
                        continue
                    else:
                        logger.error("âŒ å¤šæ¬¡é‡è¯•åä»ç„¶æ— æ³•è®¿é—®æ¨¡å‹æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–ç¨‹åºåœ¨ä½¿ç”¨æ¨¡å‹")
                        raise e
                else:
                    logger.error(f"âŒ æƒé™é”™è¯¯: {str(e)}")
                    raise e
            except Exception as e:
                error_msg = str(e)
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ–‡ä»¶æŸåæˆ–æ ¡éªŒå¤±è´¥
                if ("integrity check failed" in error_msg or 
                    "control character" in error_msg or
                    "download may be incomplete" in error_msg):
                    logger.warning(f"âš ï¸ æ£€æµ‹åˆ°æ–‡ä»¶æŸåï¼Œæ¸…ç†ç¼“å­˜åé‡è¯•: {error_msg}")
                    # æ¸…ç†ModelScopeç¼“å­˜
                    self._clear_modelscope_cache()
                
                if attempt < max_retries - 1:
                    logger.warning(f"âš ï¸ åŠ è½½å¤±è´¥ï¼Œç­‰å¾… {retry_delay} ç§’åé‡è¯•: {error_msg}")
                    time.sleep(retry_delay)
                    retry_delay *= 2
                    continue
                else:
                    logger.error(f"âŒ Qwenæ¨¡å‹ä»ModelScopeåŠ è½½å¤±è´¥: {error_msg}")
                    raise e
    
    def _clear_modelscope_cache(self):
        """æ¸…ç†ModelScopeç¼“å­˜ä¸­çš„æŸåæ–‡ä»¶"""
        try:
            import shutil
            import os
            
            # ModelScopeç¼“å­˜è·¯å¾„
            cache_dir = os.path.expanduser("~/.cache/modelscope/hub/models/qwen/Qwen2.5-1.5B-Instruct")
            temp_dir = os.path.expanduser("~/.cache/modelscope/hub/models/._____temp/qwen/Qwen2.5-1.5B-Instruct")
            
            # æ¸…ç†ä¸´æ—¶ç›®å½•
            if os.path.exists(temp_dir):
                shutil.rmtree(temp_dir)
                logger.info(f"å·²æ¸…ç†ä¸´æ—¶ç¼“å­˜ç›®å½•: {temp_dir}")
            
            # æ¸…ç†ä¸»ç¼“å­˜ç›®å½•ä¸­çš„æŸåæ–‡ä»¶
            if os.path.exists(cache_dir):
                for file_name in ["tokenizer_config.json", "tokenizer.json"]:
                    file_path = os.path.join(cache_dir, file_name)
                    if os.path.exists(file_path):
                        os.remove(file_path)
                        logger.info(f"å·²åˆ é™¤æŸåæ–‡ä»¶: {file_path}")
                        
        except Exception as e:
            logger.warning(f"æ¸…ç†ç¼“å­˜æ—¶å‡ºé”™: {str(e)}")
    
    @property
    def _llm_type(self) -> str:
        """è¿”å›LLMç±»å‹æ ‡è¯†"""
        return "qwen"
    
    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """
        è°ƒç”¨Qwenæ¨¡å‹ç”Ÿæˆæ–‡æœ¬
        
        Args:
            prompt: è¾“å…¥æç¤ºæ–‡æœ¬
            stop: åœæ­¢è¯åˆ—è¡¨
            run_manager: å›è°ƒç®¡ç†å™¨
            **kwargs: å…¶ä»–ç”Ÿæˆå‚æ•°
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬å“åº”
        """
        try:
            if self.model is None or self.tokenizer is None:
                logger.error("æ¨¡å‹æˆ–tokenizeræœªæ­£ç¡®åˆå§‹åŒ–")
                return "æ¨¡å‹æœªæ­£ç¡®åˆå§‹åŒ–"
            
            logger.info(f"æ”¶åˆ°æç¤º: {prompt[:100]}...")
            
            # æ„å»ºæ¶ˆæ¯æ ¼å¼
            messages = [
                {"role": "user", "content": prompt}
            ]
            
            # åº”ç”¨èŠå¤©æ¨¡æ¿
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            logger.info(f"æ ¼å¼åŒ–åçš„è¾“å…¥: {text[:200]}...")
            
            # ç¼–ç è¾“å…¥
            model_inputs = self.tokenizer([text], return_tensors="pt").to(self.device)
            
            # ç”Ÿæˆå“åº”
            with torch.no_grad():
                generated_ids = self.model.generate(
                    model_inputs.input_ids,
                    max_new_tokens=kwargs.get("max_length", self.max_length),
                    temperature=kwargs.get("temperature", self.temperature),
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            # è§£ç å“åº”
            generated_ids = [
                output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
            ]
            
            response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
            
            logger.info(f"ç”Ÿæˆçš„å“åº”: {response}")
            
            # æ£€æŸ¥å“åº”æ˜¯å¦ä¸ºç©º
            if not response or response.strip() == "":
                logger.warning("æ¨¡å‹ç”Ÿæˆäº†ç©ºå“åº”")
                return "æ ¹æ®æä¾›çš„æ–‡æ¡£ï¼Œæˆ‘æ— æ³•æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
            
            return response.strip()
            
        except Exception as e:
            logger.error(f"Qwenç”Ÿæˆå¤±è´¥: {str(e)}")
            return f"ç”Ÿæˆå¤±è´¥: {str(e)}"
    
    def _generate(
        self,
        prompts: List[str],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> LLMResult:
        """
        ç”Ÿæˆå¤šä¸ªæç¤ºçš„å“åº”
        
        Args:
            prompts: æç¤ºæ–‡æœ¬åˆ—è¡¨
            stop: åœæ­¢è¯åˆ—è¡¨
            run_manager: å›è°ƒç®¡ç†å™¨
            **kwargs: å…¶ä»–ç”Ÿæˆå‚æ•°
            
        Returns:
            LLMç»“æœå¯¹è±¡
        """
        generations = []
        for prompt in prompts:
            try:
                text = self._call(prompt, stop=stop, run_manager=run_manager, **kwargs)
                generations.append([Generation(text=text)])
            except Exception as e:
                logger.error(f"ç”Ÿæˆå¤±è´¥: {str(e)}")
                generations.append([Generation(text=f"ç”Ÿæˆå¤±è´¥: {str(e)}")])
        
        return LLMResult(generations=generations)
    
    def chat(
        self,
        message: str,
        history: List[tuple] = None,
        **kwargs
    ) -> str:
        """
        è¿›è¡Œå¯¹è¯
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            history: å¯¹è¯å†å²
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            æ¨¡å‹å›å¤
        """
        if history is None:
            history = []
            
        try:
            # æ„å»ºæ¶ˆæ¯å†å²
            messages = []
            for user_msg, assistant_msg in history:
                messages.append({"role": "user", "content": user_msg})
                messages.append({"role": "assistant", "content": assistant_msg})
            
            # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
            messages.append({"role": "user", "content": message})
            
            # åº”ç”¨èŠå¤©æ¨¡æ¿
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            # ç¼–ç è¾“å…¥
            model_inputs = self.tokenizer([text], return_tensors="pt").to(self.device)
            
            # ç”Ÿæˆå“åº”
            with torch.no_grad():
                generated_ids = self.model.generate(
                    model_inputs.input_ids,
                    max_new_tokens=kwargs.get("max_length", self.max_length),
                    temperature=kwargs.get("temperature", self.temperature),
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            # è§£ç å“åº”
            generated_ids = [
                output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
            ]
            
            response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
            
            return response
            
        except Exception as e:
            logger.error(f"å¯¹è¯ç”Ÿæˆå¤±è´¥: {str(e)}")
            return f"å¯¹è¯ç”Ÿæˆå¤±è´¥: {str(e)}"